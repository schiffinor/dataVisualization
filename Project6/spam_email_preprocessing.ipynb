{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roman Schiffino**\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "# Project 6 | Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Spam email preprocessing pipeline\n",
    "\n",
    "Before you can build a spam email filter, you need to transform the email data into a suitable format so that KNN or other supervised learning algorithms can process them (this is called **preprocessing**).\n",
    "\n",
    "In this project, you will work with the **Enron email dataset**, a large dataset consisting of ~34,000 emails. Enron is an energy company that famously went bankrupt in the early 2000s after committing massive accounting fraud (more info: https://en.wikipedia.org/wiki/Enron). The US government seized company emails during their investigation and they were released to the public much later and nowadays is a commonly used dataset in machine learning. \n",
    "\n",
    "Your eventual goal will be to train a supervised learning algorithm on some of the emails and predict whether the remaining ones are spam or not.\n",
    "\n",
    "But first...onto the preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall preprocessing strategy\n",
    "\n",
    "We need to turn each email's text into something an algorithm can process (**features**). We will use a simple type of feature: **bag of words counts**. That is, we will reduce an email into a vector of how many times words appeared in it.\n",
    "\n",
    "*Problem:* There are too many words across all the emails. Processing the counts in each email would take too long. For example, there are more than 40,000 words across all the emails. If we were trying to predict whether 1,000 emails are spam or not, we would need to build a `1000 x 40000` matrix (count each of the 40,000 words in each of the 1,000 emails), which would take a very long time to process by the supervised learning algorithm. \n",
    "\n",
    "A work-around that works quite well is to restrict ourselves to the most frequent $W$ words in the email dataset. You can experiment with how many words to include (e.g. as an extension), but for concreteness we will set this $W=200$ in the core project. In the above example, we can then process `1000 x 200` matrix much more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Determine email word frequency\n",
    "\n",
    "The large size of the enron email dataset makes the debugging process cumbersome. In situations like this, it is common to work with a **development dataset** â€” a mini version of the full dataset that is much faster to work with. The enron dev dataset has 2 ham emails and 3 spam emails. \n",
    "\n",
    "- Download and extract the **Enron dev** emails. You should see a base `enron` folder, with `spam` and `ham` subfolders (these are the 2 classes), and documents in each with the raw email text. There should be 2 files in the ham folder and 3 files in the spam folder.\n",
    "- In `email_preprocessor.py` implement `count_words(email_path)` to build up a python dictionary of all the words in the dataset (keys) and their associated counts (values).\n",
    "- Write `find_top_words(word_freq)` to parse the dictionary and determine the top $W$ words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import email_preprocessor as epp"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "word_freq, num_emails = epp.count_words(email_path='data/enron_dev/')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 5.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"You found {len(top_words)}/19 words.\")\n",
    "print(f\"Your top 2 words are\\n{top_words[:2]}\\nand they should be\\n['subject', 'you']\")\n",
    "print(f\"The counts of all the words are\\n{top_counts}\\nand they should be\\n[5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\")\n",
    "print(f\"The 19 words should are\\n{top_words}\\n and they should be \\n['subject', 'you', 'get', 'that', 'new', 'car', 'now', 'can', 'be', 'smart', 'love', 'ecards', 'christmas', 'tree', 'farm', 'pictures', 're', 'rankings', 'thank']\\nwith the last 17 words in any order (because their counts are tied)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Make feature vectors based only on the top word counts\n",
    "\n",
    "- Implement `make_feature_vectors`: Go back through the email folder structure and parse each email again. Now only count the frequency of words that are in the top $W$ word list. Keep track of whether each of these feature vectors are associated with a spam or not spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "hard_code_words = ['subject', 'you', 'get', 'that', 'new', 'car', 'now', 'can', 'be', 'smart', 'love', 'ecards', 'christmas', 'tree', 'farm', 'pictures', 're', 'rankings', 'thank']\n",
    "features, y = epp.make_feature_vectors(hard_code_words, num_emails, email_path='data/enron_dev/')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "firstSpamWordCounts = np.array([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "\n",
    "print(f'Your matrix of features has shape:\\n{features.shape}\\nand it should be\\n(5, 19).')\n",
    "print(f'Your class label vector has shape:\\n{y.shape}\\nand it should be\\n(5,).')\n",
    "print(\"Make sure your features have 0's and 1's in every row\")\n",
    "print(features)\n",
    "print('\\nBelow, one number should be 3, the other should be 2.')\n",
    "print(f'Number of emails of class 0: {np.sum(y == 0)}\\nNumber of emails of class 1: {np.sum(y == 1)}')\n",
    "\n",
    "inds = np.arange(len(features))\n",
    "test_ind = inds[np.all(firstSpamWordCounts == features, axis=1)]\n",
    "print(f'\\nYour vector for 2958.2004-11-03.GP.spam.txt matches expected counts?\\n{len(test_ind) == 1}\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Make train and test splits of the dataset\n",
    "\n",
    "Your matrix of features is for the entire dataset. We can't train the classifier on all these because then we won't have any emails left over to see how well your model's ability to discriminate spam/ham email generalizes to emails not seen during training!\n",
    "\n",
    "Implement `make_train_test_sets` to divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (4, 19), classes (4,)\\nTest (1, 19), classes (1,)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess full spam email dataset \n",
    "\n",
    "Now that you have tested your email preprocessing on small test cases, let's pass the actual Enron emails through your preprocessing code to create the full email dataset.\n",
    "\n",
    "Download and extract the full **Enron** emails (*zip file should be ~29MB large*). You should see a base `enron` folder, with `spam` and `ham` subfolders when you extract the zip file (these are the 2 classes).\n",
    "\n",
    "Run the test code below to check everything over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "word_freq, num_emails = epp.count_words()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 32625.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"Your top 10 words are\\n{top_words[:10]}\\nand they should be\\n['the', 'to', 'and', 'of', 'a', 'in', 'for', 'you', 'is', 'enron']\")\n",
    "print(f\"The associated counts are\\n{top_counts[:10]}\\nand they should be\\n[277459, 203659, 148873, 139578, 111796, 100961, 80765, 77592, 68097, 60852]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Make feature and class vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features, y = epp.make_feature_vectors(top_words, num_emails)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify class label coding\n",
    "\n",
    "There are\n",
    "- 16544 `ham` emails\n",
    "- 16081 `spam` emails.\n",
    "\n",
    "In the cell below, print out the number of emails that have class label `0` and the number that have class label `1`.\n",
    "- The count for class label `0` should be 16544\n",
    "- The count for class label `1` should be 16081.\n",
    "\n",
    "If the counts across the labels are reversed, recode class label `0` as `1` and class label `1` as `0` in the label vector `y`. If you do this, print out the counts for each label again and verify you get the above counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Make train and test splits of the dataset\n",
    "\n",
    "Here we divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)\n",
    "\n",
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (26100, 200), classes (26100,)\\nTest (6525, 200), classes (6525,)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Save data in binary format\n",
    "\n",
    "It adds a lot of overhead to have to run through your raw email -> train/test feature split every time you wanted to work on your project! In this step, you will export the data in memory to disk in a binary format. That way, you can quickly load all the data back into memory (directly in ndarray format) whenever you want to work with it again. No need to parse from text files!\n",
    "\n",
    "Running the following cell uses numpy's `save` function to make six files in `.npy` format (e.g. `email_train_x.npy`, `email_train_y.npy`, `email_train_inds.npy`, `email_test_x.npy`, `email_test_y.npy`, `email_test_inds.npy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.save('data/email_train_x.npy', x_train)\n",
    "np.save('data/email_train_y.npy', y_train)\n",
    "np.save('data/email_train_inds.npy', inds_train)\n",
    "np.save('data/email_test_x.npy', x_test)\n",
    "np.save('data/email_test_y.npy', y_test)\n",
    "np.save('data/email_test_inds.npy', inds_test)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
