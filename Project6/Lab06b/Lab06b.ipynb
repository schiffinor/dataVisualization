{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME**\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "CS 251: Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6b | Naive Bayes and K-Nearest Neighbor Classifiers\n",
    "\n",
    "In this lab we will be using the library scikit-learn to train a Multinomial Naive Bayes classifier and graph the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn import neighbors, naive_bayes, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Load the digits dataset\n",
    "\n",
    "We're going to be seeing if we can use Naive Bayes and K-Nearest Neighbor to classify hand writing data.  The [digits dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) is a set of 1797 8x8 pixel images, representing handwriting samples of the numbers 0-9. On your next project, you will work with another (much larger) handwriting dataset called [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "1. Load the [digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). Use the `return_X_y` parameter so that it returns both the X data and y classifications.\n",
    "2. Use [train test split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the X data and y classifications, into an X_training dataset, X_testing dataset and the corresponding y_training labels and y_testing labels.  Set the test size be `0.3` (i.e 30%) and shuffle to True.\n",
    "3. Print the shape of `X_training`, `X_testing`, `y_training`, and `y_testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "print('''\n",
    "X training data shape:  (1257, 64)\n",
    "X testing data shape:   (540, 64)\n",
    "y training labels shape:(1257,)\n",
    "y testing labels shape: (540,)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create Classifiers and Calculate Accuracy\n",
    "\n",
    "### Create a Naive Bayes Classifier\n",
    "1. Create a [Multinomial Naive Bayes Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) [(More Info)](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes). \n",
    "2. Use the fit method with the training dataset as X and the y training dataset labels as the target.\n",
    "3. Calculate the accuracy of the classifier with the test data and test dataset labels using the score method.\n",
    "4. Print the accuracy of the Naive Bayes classifier.\n",
    "\n",
    "### Create a K-NN Classifier\n",
    "1. Using the lab from last week as reference, create a [K-Nearest Neighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) [(More Info)](https://scikit-learn.org/stable/modules/neighbors.html#classification).  Set n_neighbors equal to 7.\n",
    "1. Assign your classifier to a variable with a **different** name than your Naive Bayes classifier.  \n",
    "2. Use the fit method with the training dataset as X and the y training dataset labels as the target.\n",
    "3. Calculate the accuracy of the classifier with the test data and test dataset labels using the score method.\n",
    "4. Print the accuracy of the K-NN classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('''\n",
    "Multinomial Naive Bayes Classifier Accuracy: 0.89444...\n",
    "K-Nearest Neighbor Classifier Accuracy:   0.99074...\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create a confusion matrix for each classifier\n",
    "\n",
    "1. Find the predicted labels for the X test data using the predict method for the Naive Bayes classifier and K-NN classifier.\n",
    "1. Create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for each classifier, using the actual labels and predicted labels.  \n",
    "2. Print the confusion matrices. If everything is working as expected, you should see vast majority of numbers along the main diagonal (*going from the top-left to the bottom-right of the matrix*).\n",
    "3. Visualize the confusion matrices using imshow. For reference, use Lab 4a and this [matplotlib example](https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html) of an annotated heatmap.\n",
    "    1. Set x_ticks and y_ticks to align with the list of digits.\n",
    "    2. Use imshow to draw the matrix\n",
    "    3. Choose a perceptually uniform, sequential [colormap](https://matplotlib.org/tutorials/colors/colormaps.html)\n",
    "    4. Use a colorbar to label the matrix\n",
    "    5. Remember to call `plt.show()` at the end, or other plots later might not work.\n",
    "    6. Give your plots meaningful titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('''\n",
    "K-Nearest Neighbor Confusion Matrix\n",
    " [[53  0  0  0  0  0  0  0  0  0]\n",
    " [ 0 50  0  0  0  0  0  0  0  0]\n",
    " [ 0  0 47  0  0  0  0  0  0  0]\n",
    " [ 0  0  0 54  0  0  0  0  0  0]\n",
    " [ 0  0  0  0 60  0  0  0  0  0]\n",
    " [ 0  0  0  0  0 64  1  0  0  1]\n",
    " [ 0  0  0  0  0  0 53  0  0  0]\n",
    " [ 0  0  0  0  0  0  0 55  0  0]\n",
    " [ 0  0  0  0  0  0  0  0 43  0]\n",
    " [ 0  0  0  1  1  1  0  0  0 56]]\n",
    " Multinomial Naive Bayes Confusion Matrix\n",
    "[[52  0  0  0  1  0  0  0  0  0]\n",
    " [ 0 33  7  0  0  0  0  0  6  4]\n",
    " [ 0  1 44  0  0  0  0  0  2  0]\n",
    " [ 0  0  1 46  0  0  0  1  4  2]\n",
    " [ 0  0  0  0 60  0  0  0  0  0]\n",
    " [ 0  0  0  0  1 51  1  0  0 13]\n",
    " [ 0  0  0  0  1  0 51  0  1  0]\n",
    " [ 0  0  0  0  0  0  0 55  0  0]\n",
    " [ 0  2  0  0  0  0  0  0 41  0]\n",
    " [ 0  0  0  0  1  1  0  4  3 50]]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Which digits are most likely to be misclassified and what are they most likely to be misclassified as?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
