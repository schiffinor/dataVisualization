{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "# Project 6 | Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.438597Z",
     "start_time": "2024-06-21T16:05:16.858261Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Naive Bayes Classifier\n",
    "\n",
    "After finishing your email preprocessing pipeline, implement the one other supervised learning algorithm we we will use to classify email, **Naive Bayes**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Implement Naive Bayes\n",
    "\n",
    "In `naive_bayes.py`, implement the following methods:\n",
    "- Constructor\n",
    "- get methods\n",
    "- `train(data, y)`: Train the Naive Bayes classifier so that it records the \"statistics\" of the training set: class priors (i.e. how likely an email is in the training set to be spam or ham?) and the class likelihoods (the probability of a word appearing in each class — spam or ham).\n",
    "- `predict(data)`: Combine the class likelihoods and priors to compute the posterior distribution. The predicted class for a test sample is the class that yields the highest posterior probability.\n",
    "\n",
    "\n",
    "#### Bayes rule ingredients: Priors and likelihood (`train`)\n",
    "\n",
    "To compute class predictions (probability that a test example belong to either spam or ham classes), we need to evaluate **Bayes Rule**. This means computing the priors and likelihoods based on the training data.\n",
    "\n",
    "**Prior:** $$P_c = \\frac{N_c}{N}$$ where $P_c$ is the prior for class $c$ (spam or ham), $N_c$ is the number of training samples that belong to class $c$ and $N$ is the total number of training samples.\n",
    "\n",
    "**Likelihood:** $$L_{c,w} = \\frac{T_{c,w} + 1}{T_{c} + M}$$ where\n",
    "- $L_{c,w}$ is the likelihood that word $w$ belongs to class $c$ (*i.e. what we are solving for*)\n",
    "- $T_{c,w}$ is the total count of **word $w$** in emails that are only in class $c$ (*either spam or ham*)\n",
    "- $T_{c}$ is the total count of **all words** that appear in emails of the class $c$ (*total number of words in all spam emails or total number of words in all ham emails*)\n",
    "- $M$ is the number of features (*number of top words*).\n",
    "\n",
    "#### Bayes rule ingredients: Posterior (`predict`)\n",
    "\n",
    "To make predictions, we now combine the prior and likelihood to get the posterior:\n",
    "\n",
    "**Log Posterior:** $$Log(\\text{Post}_{i, c}) = Log(P_c) + \\sum_{j \\in J_i}x_{i,j}Log(L_{c,j})$$\n",
    "\n",
    " where\n",
    "- $\\text{Post}_{i,c}$ is the posterior for class $c$ for test sample $i$(*i.e. evidence that email $i$ is spam or ham*). We solve for its logarithm.\n",
    "- $Log(P_c)$ is the logarithm of the prior for class $c$.\n",
    "- $x_{i,j}$ is the number of times the jth word appears in the ith email.\n",
    "- $Log(L_{c,j})$: is the log-likelihood of the jth word in class $c$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.482133Z",
     "start_time": "2024-06-21T16:05:18.439607Z"
    }
   },
   "source": [
    "from naive_bayes import NaiveBayes"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `train`\n",
    "\n",
    "###### Class priors and likelihoods\n",
    "\n",
    "The following test should be used only if storing the class priors and likelihoods directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.537006Z",
     "start_time": "2024-06-21T16:05:18.483138Z"
    }
   },
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.randint(low=0, high=20, size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your class priors are: {nbc.get_priors()}\\nand should be          [0.28 0.22 0.32 0.18].')\n",
    "print(f'Your class likelihoods shape is {nbc.get_likelihoods().shape} and should be (4, 6).')\n",
    "print(f'Your likelihoods are:\\n{nbc.get_likelihoods()}')\n",
    "\n",
    "print('and should be')\n",
    "print('''[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
    " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
    " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
    " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]''')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your class priors are: [-1.27297 -1.51413 -1.13943 -1.7148 ]\n",
      "and should be          [0.28 0.22 0.32 0.18].\n",
      "Your class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your likelihoods are:\n",
      "[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
      " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
      " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
      " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]\n",
      "and should be\n",
      "[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
      " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
      " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
      " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Log of class priors and likelihoods\n",
    "\n",
    "This test should be used only if storing the log of the class priors and likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.581062Z",
     "start_time": "2024-06-21T16:05:18.538014Z"
    }
   },
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.randint(low=0, high=20, size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your log class priors are: {nbc.get_priors()}\\nand should be              [-1.27297 -1.51413 -1.13943 -1.7148 ].')\n",
    "print(f'Your log class likelihoods shape is {nbc.get_likelihoods().shape} and should be (4, 6).')\n",
    "print(f'Your log likelihoods are:\\n{nbc.get_likelihoods()}')\n",
    "\n",
    "\n",
    "print('and should be')\n",
    "print('''[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
    " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
    " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
    " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]''')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your log class priors are: [-1.27297 -1.51413 -1.13943 -1.7148 ]\n",
      "and should be              [-1.27297 -1.51413 -1.13943 -1.7148 ].\n",
      "Your log class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your log likelihoods are:\n",
      "[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
      " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
      " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
      " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]\n",
      "and should be\n",
      "[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
      " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
      " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
      " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.624745Z",
     "start_time": "2024-06-21T16:05:18.582077Z"
    }
   },
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_train = np.random.randint(low=0, high=15, size=(100, 10))\n",
    "data_test = np.random.randint(low=0, high=15, size=(15, 10))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_train, y_test)\n",
    "test_y_pred = nbc.predict(data_test)\n",
    "\n",
    "print(f'Your predicted classes are\\n{test_y_pred}\\nand should be\\n[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]]')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your predicted classes are\n",
      "[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]\n",
      "and should be\n",
      "[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Spam filtering\n",
    "\n",
    "Use your Naive Bayes classifier to predict whether emails in the Enron email dataset are spam! Start by running the following code that uses `np.load` to load in the train/test split that you created last week.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.720157Z",
     "start_time": "2024-06-21T16:05:18.625753Z"
    }
   },
   "source": [
    "x_train = np.load('data/email_train_x.npy')\n",
    "y_train = np.load('data/email_train_y.npy')\n",
    "inds_train = np.load('data/email_train_inds.npy')\n",
    "x_test = np.load('data/email_test_x.npy')\n",
    "y_test = np.load('data/email_test_y.npy')\n",
    "inds_test = np.load('data/email_test_inds.npy')"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.779761Z",
     "start_time": "2024-06-21T16:05:18.721163Z"
    }
   },
   "source": [
    "# Check the shapes of the data\n",
    "print(x_train.shape, y_train.shape, inds_train.shape)\n",
    "print(x_test.shape, y_test.shape, inds_test.shape)\n",
    "\n",
    "# Check the first few entries of the training data\n",
    "print(x_train[:5])\n",
    "\n",
    "# Check the first few entries of the training labels\n",
    "print(y_train[:5])\n",
    "\n",
    "# Check the first few entries of the training indices\n",
    "print(inds_train[:5])\n",
    "\n",
    "# Check the first few entries of the test data\n",
    "print(x_test[:5])\n",
    "\n",
    "# Check the first few entries of the test labels\n",
    "print(y_test[:5])\n",
    "\n",
    "# Check the first few entries of the test indices\n",
    "print(inds_test[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26100, 200) (26100,) (26100,)\n",
      "(6525, 200) (6525,) (6525,)\n",
      "[[ 3.  7.  1.  1.  2.  3.  1.  3.  3.  4.  5.  1.  4.  0.  6.  4.  2.  2.\n",
      "   0.  0.  0.  3.  0.  0.  1. 11.  0.  0.  2.  1.  0.  0.  1.  0.  0.  1.\n",
      "   0.  0.  1.  1.  6.  2.  0.  0.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "   1.  0.  0.  4.  0.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  2.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  5.  0.  2.  0.  0.  2.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  1.  0.  0.  1.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 7.  0.  3.  3.  0.  2.  2.  3.  0.  1.  0.  1.  1.  0.  1.  1.  0.  0.\n",
      "   0.  1.  2.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  2.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  4.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 7.  1.  4.  2.  7.  1.  7.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.\n",
      "   2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.\n",
      "   1.  0.  2.  0.  0.  0.  1.  0.  0.  1.  3.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 1.  1.  1.  1.  9.  1.  1.  0.  5.  0.  3.  0.  3.  0.  1.  1.  0.  1.\n",
      "   2.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  2.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0.  0.\n",
      "   0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  8.  0.  0.  0.  0.  0.  2.  0.  0.  1.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0. 10.  0.  0.  0.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.\n",
      "   0.  0.]]\n",
      "[0. 0. 0. 0. 1.]\n",
      "[ 9759  5093 13639 16491 26618]\n",
      "[[ 0.  1.  1.  0.  0.  0.  2.  2.  0.  3.  2.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  3.  0.  0.\n",
      "   1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [14. 20. 13. 27.  9.  6.  4. 11.  5.  0.  5.  2.  0.  4.  2.  1.  4.  1.\n",
      "  18.  5.  3.  6.  2.  0.  3.  0.  4.  2.  0.  2.  1.  7.  0.  0.  0.  1.\n",
      "   2.  0.  2.  0.  0.  0.  0.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  2.  0.  3.  0.  0.  0.  0.  2.  0.  3.  2.  0.  1.  1.  0.\n",
      "   0.  4.  0.  0.  3.  0.  0.  0.  0.  0.  0.  1.  2.  1.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  4.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0.  0.\n",
      "   0.  0.  2.  0.  0.  0.  0.  0.  2.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  3.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  4.  0.  0.  0.  0.\n",
      "   0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  2.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.\n",
      "   0.  1.  0.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  1.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "[0. 1. 1. 1. 1.]\n",
      "[11903 17632 27470 26954 23385]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.937270Z",
     "start_time": "2024-06-21T16:05:18.780768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a Naive Bayes classifier\n",
    "nbc = NaiveBayes(num_classes=2)\n",
    "\n",
    "# Train the classifier\n",
    "nbc.train(x_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = nbc.predict(x_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Your Naive Bayes classifier has an accuracy of {accuracy:.3f}.')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Naive Bayes classifier has an accuracy of 0.896.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Questions\n",
    "\n",
    "**Question 7:** What accuracy do you get on the test set with Naive Bayes. It should be roughly 89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Confusion matrix\n",
    "\n",
    "To get a better sense of the errors that the Naive Bayes classifier makes, implement the `confusion_matrix` method in `Classifer` then create a confusion matrix of the spam classification results. Assign the confusion matrix below to the variable `conf_matrix_nb` and run the code below to help test your confusion matrix."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:18.992169Z",
     "start_time": "2024-06-21T16:05:18.938278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conf_matrix_nb = nbc.confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix_nb)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2790  491]\n",
      " [ 186 3058]]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:19.048483Z",
     "start_time": "2024-06-21T16:05:18.994178Z"
    }
   },
   "source": [
    "print(f'The total number of entries in your confusion matrix is {int(conf_matrix_nb.sum())} and should be {len(y_test)}.')\n",
    "print(f'The total number of ham entries in your confusion matrix is {int(conf_matrix_nb[0].sum())} and should be {int(np.sum(y_test == 0))}.')\n",
    "print(f'The total number of spam entries in your confusion matrix is {int(conf_matrix_nb[1].sum())} and should be {int(np.sum(y_test == 1))}.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of entries in your confusion matrix is 6525 and should be 6525.\n",
      "The total number of ham entries in your confusion matrix is 3281 and should be 3281.\n",
      "The total number of spam entries in your confusion matrix is 3244 and should be 3244.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e. Questions\n",
    "\n",
    "**Question 8:** Interpret the confusion matrix, using the convention that positive detection means spam (*e.g. a false positive means classifying a ham email as spam*). What types of errors are made more frequently by the classifier? What does this mean (*i.e. X (spam/ham) is more likely to be classified than Y (spam/ham) than the other way around*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Comparison with KNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:05:19.100203Z",
     "start_time": "2024-06-21T16:05:19.049492Z"
    }
   },
   "source": "from knn import KNN",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. KNN spam email classification accuracy\n",
    "Run a similar analysis to what you did with Naive Bayes above. When computing accuracy on the test set, you may want to reduce the size of the test set (e.g. to the first 500 emails in the test set)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:09:53.003462Z",
     "start_time": "2024-06-21T16:05:19.101209Z"
    }
   },
   "source": [
    "# Create a KNN classifier\n",
    "knn = KNN(num_classes=2)\n",
    "\n",
    "# Train the classifier\n",
    "knn.train(x_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred_knn = knn.predict(x_test, 3)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6525,) (500,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m y_pred_knn \u001B[38;5;241m=\u001B[39m knn\u001B[38;5;241m.\u001B[39mpredict(x_test, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Calculate the accuracy\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m accuracy_knn \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(\u001B[43my_pred_knn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYour KNN classifier has an accuracy of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy_knn\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (6525,) (500,) "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T17:26:46.489405Z",
     "start_time": "2024-06-21T17:26:46.437463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the accuracy\n",
    "accuracy_knn = np.mean(y_pred_knn == y_test)\n",
    "\n",
    "print(f'Your KNN classifier has an accuracy of {accuracy_knn:.3f}.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your KNN classifier has an accuracy of 0.917.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. KNN spam email confusion matrix\n",
    "\n",
    "In the cell below, create a confusion matrix for your KNN classifier results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T17:26:43.465631Z",
     "start_time": "2024-06-21T17:26:43.412004Z"
    }
   },
   "source": [
    "# Create the confusion matrix\n",
    "conf_matrix_knn = knn.confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2964  317]\n",
      " [ 222 3022]]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** What accuracy did you get on the test set (potentially reduced in size)?\n",
    "\n",
    "**Question 10:** How does the confusion matrix compare to that obtained by Naive Bayes (*If you reduced the test set size, keep that in mind*)?\n",
    "\n",
    "**Question 11:** What is the primary benefit and downside of KNN that we observe here compared to Naive Bayes?\n",
    "\n",
    "**Question 12:** When potentially reducing the size of the test set here, why is it important that we shuffled our train and test set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:** \n",
    "\n",
    "**Answer 10:**\n",
    "\n",
    "**Answer 11:** \n",
    "\n",
    "**Answer 12:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### a. Reminder: AI Policy\n",
    "\n",
    "The goal of extensions is to learn and create something new beyond the base project that excites you. To serve this goal and out of fairness to your hardworking classmates, **AI (e.g. ChatGPT, Copilot, etc.) should NOT be used in ANY way on this project and extensions.** This includes both written analysis, plotting, and code. We will only grade **your** work, not an AI's. **We will stop grading your project if we notice AI-generated content (to any capacity).**\n",
    "\n",
    "### b. Guidelines\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "1. Not modify / prevent any code from the core project from working (e.g. make a copy before changing). In other words, **the notebook test code should still work!**\n",
    "2. **You must describe what you did and what you found in detail**. This includes a summary of parameter values used in your simulations.\n",
    "3. Include (*labeled!*) plots and/or numbers to present your results.\n",
    "4. Write up your extensions below or in a separate notebook.\n",
    "5. Give kudos to all sources, including anyone that you consulted.\n",
    "\n",
    "### c. Suggestions\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**\n",
    "\n",
    "The ideas below are **suggested** extensions — feel free to go in another direction related to this project that is not listed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Classify your own datasets\n",
    "\n",
    "- Find datasets that you find interesting and run classification on them using your KNN algorithm (and if applicable, Naive Bayes). Analysis the performance of your classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distance metrics\n",
    "- Compare KNN performance with the $L^2$ and $L^1$ distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Fold Cross-Validation\n",
    "\n",
    "- Research this technique and apply it to data and your KNN and/or Naive Bayes classifiers.\n",
    "\n",
    "*For credit: You need to provide exact links to resources you relied on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Email error analysis\n",
    "\n",
    "- Dive deeper into the properties of the emails that were misclassified (FP and/or FN) by Naive Bayes or KNN. What is their word composition? How many words were skipped because they were not in the training set? What could plausibly account for the misclassifications?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Investigate the misclassification errors\n",
    "\n",
    "Numbers are nice, but they may not the best for developing your intuition. Sometimes, you want to see what an misclassification *actually looks like* to help you improve your algorithm. Retrieve the actual text of some example emails of false positive and false negative misclassifications to see if helps you understand why the misclassification occurred. Here is an example workflow:\n",
    "\n",
    "- Decide on how many FP and FN emails you would like to retrieve. Find the indices of this many false positive and false negative misclassification. Remember to use your `test_inds` array to look up the index of the emails BEFORE shuffling happened.\n",
    "- Implement the function `retrieve_emails` in `email_preprocessor.py` to return the string of the raw email at the error indices.\n",
    "- Call your function to print out the emails that produced misclassifications.\n",
    "\n",
    "Do the FP and FN emails make sense? Why? Do the emails have properties in common? Can you quantify and interpret them?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
